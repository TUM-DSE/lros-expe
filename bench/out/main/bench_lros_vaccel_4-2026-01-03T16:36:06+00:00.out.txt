[    0.006326] ERR:  [libvirtio_bus] <virtio_bus.c @  141> Failed to find the driver for the virtio device 0x4000a4020 (id:1)
[    0.007539] ERR:  [libvirtio_pci] <virtio_pci.c @  426> Failed to register the virtio device: -14
[    0.008437] ERR:  [libukbus_pci] <pci_bus.c @  111> PCI 00:01.00: Failed to initialize device driver
Powered by
o.   .o       _ _               __ _
Oo   Oo  ___ (_) | __ __  __ _ ' _) :_
oO   oO ' _ `| | |/ /  _)' _` | |_|  _)
oOo oOO| | | | |   (| | | (_) |  _) :_
 OoOoO ._, ._:_:_,\_._,  .__,_:_, \___)
                    Pan 0.19.1~58c46f99
register_backend: registered backend VACCEL (1 devices)
register_device: registered device VACCEL (VACCEL (adapted from RKNN))
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
ml initializing
build: 16 (75774ce) with gcc (GCC) 13.3.0 for aarch64-unknown-linux-gnu (debug)
params devices: null
llama_model_load_from_file_impl: using device VACCEL (VACCEL (adapted from RKNN)) - 16361 MiB free
llama_model_loader: loaded meta data with 31 key-value pairs and 147 tensors from /root/models/Llama-3.2-1B-Instruct-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                            general.license str              = llama3.2
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 16
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  19:                          general.file_type u32              = 1
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type  f16:  113 tensors
ml initialized
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 2.30 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
model loaded
stats loaded
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
info printed
load_tensors: loading model tensors, this can take a while... (mmap = false)
make_cpu_buft_list: disabling extra buffer types (i.e. repacking) since a GPU device is available
load_tensors: layer   0 assigned to device VACCEL
load_tensors: layer   1 assigned to device VACCEL
load_tensors: layer   2 assigned to device VACCEL
load_tensors: layer   3 assigned to device VACCEL
load_tensors: layer   4 assigned to device VACCEL
load_tensors: layer   5 assigned to device VACCEL
load_tensors: layer   6 assigned to device VACCEL
load_tensors: layer   7 assigned to device VACCEL
load_tensors: layer   8 assigned to device VACCEL
load_tensors: layer   9 assigned to device VACCEL
load_tensors: layer  10 assigned to device VACCEL
load_tensors: layer  11 assigned to device VACCEL
load_tensors: layer  12 assigned to device VACCEL
load_tensors: layer  13 assigned to device VACCEL
load_tensors: layer  14 assigned to device VACCEL
load_tensors: layer  15 assigned to device VACCEL
load_tensors: layer  16 assigned to device VACCEL
load_tensors: done getting tensors
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:          CPU model buffer size =  2357.26 MiB
..............................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 1024
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (1024) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     1.96 MiB
init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1
init:        CPU KV buffer size =   128.00 MiB
llama_context: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB
model.n_devices() = 1
model.params.n_gpu_layers = 20
model.hparams.n_layer = 16
model.params.split_mode = 1
cparams.offload_kqv = 1
pipeline_parallel = 0
llama_context:        CPU compute buffer size =   280.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 1

main: n_kv_max = 4096, n_batch = 2048, n_ubatch = 512, flash_attn = 0, is_pp_shared = 0, n_gpu_layers = 20, n_threads = 1, n_threads_batch = 1

|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |
|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|
|    16 |    128 |    4 |    576 |   17.279 |     3.70 |   48.551 |    10.55 |   65.830 |     8.75 |
|    16 |    256 |    4 |   1088 |   17.282 |     3.70 |  103.225 |     9.92 |  120.507 |     9.03 |
|    32 |    128 |    4 |    640 |   34.732 |     3.69 |   47.891 |    10.69 |   82.622 |     7.75 |
|    32 |    256 |    4 |   1152 |   34.686 |     3.69 |  105.631 |     9.69 |  140.318 |     8.21 |

llama_perf_context_print:        load time =   10322.51 ms
llama_perf_context_print: prompt eval time =       0.00 ms /  3472 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =  419607.99 ms /  3473 tokens


